{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1. Import libraries \n",
    "2. Load dataset\n",
    "3. Make a copy of original dataset and use one for continued pre-processing\n",
    "4. Inspect the data | Check for inconsistencies and missing values, duplicates\n",
    "5. Split the data set in train/test\n",
    "6. Separate the categorical and numerical features\n",
    "7. Check for missing and handle missing values in categorical and numerical using simple imputer\n",
    "8. Fit model and evaluate it"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn import set_config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import make_column_selector\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "set_config(display='diagram')\n",
    "# Set the maximum number of rows and columns to display\n",
    "pd.set_option('display.max_rows', 10)      # Set the maximum number of rows to display\n",
    "# pd.set_option('display.max_columns', 5)   # Set the maximum number of columns to display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159 entries, 0 to 158\n",
      "Data columns (total 7 columns):\n",
      " #   Column   Non-Null Count  Dtype  \n",
      "---  ------   --------------  -----  \n",
      " 0   Species  155 non-null    object \n",
      " 1   Weight   159 non-null    float64\n",
      " 2   Length1  157 non-null    float64\n",
      " 3   Length2  157 non-null    float64\n",
      " 4   Length3  150 non-null    float64\n",
      " 5   Height   156 non-null    float64\n",
      " 6   Width    157 non-null    float64\n",
      "dtypes: float64(6), object(1)\n",
      "memory usage: 8.8+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Length1</th>\n",
       "      <th>Length2</th>\n",
       "      <th>Length3</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bream</td>\n",
       "      <td>242.0</td>\n",
       "      <td>23.2</td>\n",
       "      <td>25.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.5200</td>\n",
       "      <td>4.0200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bream</td>\n",
       "      <td>290.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>31.2</td>\n",
       "      <td>12.4800</td>\n",
       "      <td>4.3056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bream</td>\n",
       "      <td>340.0</td>\n",
       "      <td>23.9</td>\n",
       "      <td>26.5</td>\n",
       "      <td>31.1</td>\n",
       "      <td>12.3778</td>\n",
       "      <td>4.6961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bream</td>\n",
       "      <td>363.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>12.7300</td>\n",
       "      <td>4.4555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bream</td>\n",
       "      <td>430.0</td>\n",
       "      <td>26.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>12.4440</td>\n",
       "      <td>5.1340</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Species  Weight  Length1  Length2  Length3   Height   Width\n",
       "0   Bream   242.0     23.2     25.4     30.0  11.5200  4.0200\n",
       "1   Bream   290.0     24.0     26.3     31.2  12.4800  4.3056\n",
       "2   Bream   340.0     23.9     26.5     31.1  12.3778  4.6961\n",
       "3   Bream   363.0     26.3     29.0     33.5  12.7300  4.4555\n",
       "4   Bream   430.0     26.5     29.0     34.0  12.4440  5.1340"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './Fish - Fish.csv'\n",
    "fish_df = pd.read_csv(path)\n",
    "fish_df.info()\n",
    "fish_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fish_df_copy = fish_df.copy()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Inspect the Data\n",
    "\n",
    "    There are 0 duplicates.  However, there seems to be some null values present based on initial observance of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159, 7)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reorder columns so that it is easier to search for target \n",
    "column_reordered = ['Species', 'Length1', 'Length2', 'Length3', 'Height', 'Width', 'Weight']\n",
    "\n",
    "fish_df = fish_df[column_reordered]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "      <th>Length1</th>\n",
       "      <th>Length2</th>\n",
       "      <th>Length3</th>\n",
       "      <th>Height</th>\n",
       "      <th>Width</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bream</td>\n",
       "      <td>23.2</td>\n",
       "      <td>25.4</td>\n",
       "      <td>30.0</td>\n",
       "      <td>11.5200</td>\n",
       "      <td>4.0200</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bream</td>\n",
       "      <td>24.0</td>\n",
       "      <td>26.3</td>\n",
       "      <td>31.2</td>\n",
       "      <td>12.4800</td>\n",
       "      <td>4.3056</td>\n",
       "      <td>290.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bream</td>\n",
       "      <td>23.9</td>\n",
       "      <td>26.5</td>\n",
       "      <td>31.1</td>\n",
       "      <td>12.3778</td>\n",
       "      <td>4.6961</td>\n",
       "      <td>340.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bream</td>\n",
       "      <td>26.3</td>\n",
       "      <td>29.0</td>\n",
       "      <td>33.5</td>\n",
       "      <td>12.7300</td>\n",
       "      <td>4.4555</td>\n",
       "      <td>363.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bream</td>\n",
       "      <td>26.5</td>\n",
       "      <td>29.0</td>\n",
       "      <td>34.0</td>\n",
       "      <td>12.4440</td>\n",
       "      <td>5.1340</td>\n",
       "      <td>430.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Species  Length1  Length2  Length3   Height   Width  Weight\n",
       "0   Bream     23.2     25.4     30.0  11.5200  4.0200   242.0\n",
       "1   Bream     24.0     26.3     31.2  12.4800  4.3056   290.0\n",
       "2   Bream     23.9     26.5     31.1  12.3778  4.6961   340.0\n",
       "3   Bream     26.3     29.0     33.5  12.7300  4.4555   363.0\n",
       "4   Bream     26.5     29.0     34.0  12.4440  5.1340   430.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fish_df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "159 rows, 9 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking for duplicates ===>  \n",
      " 0\n",
      "checking for null values ===> \n",
      " Species    4\n",
      "Length1    2\n",
      "Length2    2\n",
      "Length3    9\n",
      "Height     3\n",
      "Width      2\n",
      "Weight     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def chk_for_dups_null(df):\n",
    "    print(\"checking for duplicates ===>  \\n\",df.duplicated().sum())\n",
    "    print(\"checking for null values ===> \\n\",df.isna().sum())\n",
    "chk_for_dups_null(fish_df)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there are null values. We could drop them since they're 5% less of the dataframe, but we are practicing pipelines and we will create a pipeline for filling in missing data and standardizing."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Split the Dataset\n",
    "\n",
    "    Why split data:\n",
    "            Training Data: This is the portion of your data used to train your machine learning model. The model learns patterns, relationships, and rules from this data.\n",
    "            Testing Data: This is a separate portion of your data that the model has never seen during training. It's used to assess how well the trained model generalizes to new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Weight'\n",
    "X = fish_df.drop(columns=[target])\n",
    "y= fish_df[target]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    The objects returned by the train_test_split function are typically NumPy arrays or pandas DataFrames (depending on the data types of the input data). These objects are also considered data structures or data containers.\n",
    "    \n",
    "    X_train, X_test, y_train, and y_test are data containers (objects) that hold your feature and target data. The specific data type of these objects depends on the data types of your input data when calling train_test_split.\n",
    "    \n",
    "    X_train: Contains the feature data for the training set. It's typically a DataFrame or NumPy array. It's the portion of your data that your machine learning model will be trained on.\n",
    "\n",
    "    X_test: Contains the feature data for the testing set. It's also typically a DataFrame or NumPy array. It's used to evaluate the model's performance on unseen data.\n",
    "\n",
    "    y_train: Contains the target variable data for the training set. It's typically a Series (if using pandas) or a NumPy array.\n",
    "\n",
    "    y_test: Contains the target variable data for the testing set. It's also typically a Series (if using pandas) or a NumPy array.\n",
    "\n",
    "    The specific order of these variables (X_train, X_test, y_train, y_test) is a convention often used in machine learning to make it clear which sets correspond to feature data and target data. It's also the order in which the train_test_split function returns the values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6 : Separate categorical and numerical data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the columns selectors\n",
    "num_selector = make_column_selector(dtype_include='number')\n",
    "cat_selector = make_column_selector(dtype_include='object')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns are : ['Length1', 'Length2', 'Length3', 'Height', 'Width'] \n",
      " Categorical columns are : ['Species']\n"
     ]
    }
   ],
   "source": [
    "numeric_columns = num_selector(X_train)\n",
    "categorical_columns = cat_selector(X_train)\n",
    "print(f\"Numeric columns are : {numeric_columns} \\n Categorical columns are : {categorical_columns}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Handle Missing Data, Categorical Data, and Scaling!\n",
    "\n",
    "    AKA setting up our preprocessor pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler\n",
    "scaler = StandardScaler() #for numeric data \n",
    "# One-hot encoder\n",
    "ohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False) #categorical nominal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a transformer for imputing missing values in numeric columns\n",
    "# \"imputer\" can be any name just as long as it makes sense\n",
    "\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', scaler)\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', ohe)\n",
    "])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numeric_transformer and categorical_transformer are instances of a data preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a ColumnTransformer to apply the numeric transformer to numeric columns\n",
    "# \"num\" and \"cat\" can be any name just as long as it makes sense\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns)\n",
    "    ])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important note: the next few cells testing what the data looks like before train/test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit and transform your data\n",
    "X_preprocessed = preprocessor.fit_transform(X_train)\n",
    "\n",
    "# Print intermediate results (optional)\n",
    "print(\"Intermediate Result after Numeric Imputation:\")\n",
    "print(X_preprocessed[:, :len(numeric_columns)])  # Numeric columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Intermediate Result after Categorical Imputation:\")\n",
    "print(X_preprocessed[:, len(numeric_columns):])  # Categorical columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check what's happening under the hood in scikit-learn's transformers and pipelines, you can use the fit_transform or transform methods and print the intermediate results at various stages. This can help you understand how data is being processed at each step. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data being returned from this preprocessor are Numpy arrays,\n",
    "\n",
    "Turning the data into a NumPy array in this context has several advantages and use cases:\n",
    "\n",
    "    Compatibility with Scikit-Learn: Scikit-Learn, a popular machine learning library, primarily works with NumPy arrays or pandas DataFrames. By converting your data into a NumPy array, you ensure that it can be seamlessly integrated with various Scikit-Learn functions and models.\n",
    "\n",
    "    Efficient Computation: NumPy arrays are highly optimized for numerical computations. When you perform mathematical operations or apply machine learning algorithms, NumPy arrays are more efficient than native Python data structures like lists or tuples.\n",
    "\n",
    "    Slicing and Indexing: NumPy arrays allow for easy slicing and indexing, as you've seen in the code. This makes it convenient to access specific subsets of your data, such as numeric and categorical columns in this case.\n",
    "\n",
    "    Standardized Data Format: Converting data into a NumPy array helps standardize the data format. This can be important when working with various data preprocessing steps, transformations, and machine learning models. It ensures a consistent and expected input format.\n",
    "\n",
    "    Memory Efficiency: NumPy arrays can be more memory-efficient than pandas DataFrames, especially when dealing with large datasets. NumPy stores data in a more compact way, which can reduce memory overhead.\n",
    "\n",
    "    Integration with Other Libraries: Many other scientific and numerical libraries, beyond Scikit-Learn, are built to work with NumPy arrays. This compatibility makes it easier to use these libraries in conjunction with your machine learning workflow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important to note:\n",
    "The missing values and onehot encoder are filled in but NOT applied to the original dataframe  (fish_df). The code I provided earlier filled in missing values and stored the result in the X_preprocessed variable, which is a NumPy array.\n",
    "\n",
    "X_preprocessed now contains the data with missing values filled in and all columns converted to numerical format.\n",
    "\n",
    "    Missing Values: The missing values in both numeric and categorical columns were filled in using the appropriate strategies (mean for numeric and most frequent for categorical).\n",
    "\n",
    "    One-Hot Encoding: Categorical columns were one-hot encoded, converting them from categorical to numerical format.\n",
    "\n",
    "    Numerical Columns: Numeric columns remained unchanged but now contain no missing values."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use the StandardScaler to standardize your numeric features so that they have a mean of 0 and a standard deviation of 1. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why do we scale, again?\n",
    "\n",
    "    Equal Treatment of Features: Scaling ensures that all features are treated equally in terms of their impact on the model. Without scaling, features with larger scales (e.g., income in thousands) can dominate those with smaller scales (e.g., age) in the learning process.\n",
    "\n",
    "    Convergence: Algorithms that rely on optimization, like gradient descent, converge faster when features are on similar scales. This helps reduce the time it takes for the model to find the optimal solution.\n",
    "\n",
    "    Numerical Stability: Scaling can improve the numerical stability of certain calculations, particularly in algorithms that involve matrix operations. This prevents issues like overflow or underflow.\n",
    "\n",
    "Mathematics of Scaling:\n",
    "Scaling involves transforming your data so that it has a particular scale, often with a mean of 0 and a standard deviation of 1 (a standard normal distribution). The standard scaling transformation is:\n",
    "\n",
    "    Standardized Value  =   Value−MeanStandard Deviation\n",
    "                           ________________________________\n",
    "\n",
    "                            Standard DeviationValue−Mean​\n",
    "\n",
    "Here's a brief explanation of the math:\n",
    "\n",
    "    Subtracting the mean centers the data around 0. This step ensures that the transformed data has a mean of 0.\n",
    "\n",
    "    Dividing by the standard deviation scales the data. It makes the variance of the transformed data equal to 1. This step ensures that the transformed data has a standard deviation of 1.\n",
    "\n",
    "By scaling your data using the standardization process, you bring all features to a similar scale, and they all have a mean of 0 and a standard deviation of 1. This standardization helps ensure that your machine learning algorithms can effectively learn from and generalize to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Bream</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>Pike</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>Smelt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>Perch</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>Parkki</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Species\n",
       "26    Bream\n",
       "137    Pike\n",
       "146   Smelt\n",
       "90    Perch\n",
       "66   Parkki"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a subset of data for only categorical columns\n",
    "train_cat_data = X_train[cat_selector(X_train)]\n",
    "test_cat_data = X_test[cat_selector(X_test)]\n",
    "train_cat_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 1., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the OneHotEncoder on the training data\n",
    "ohe.fit(train_cat_data)\n",
    "#transform both the training and the testing data\n",
    "train_ohe = ohe.transform(train_cat_data)\n",
    "test_ohe = ohe.transform(test_cat_data)\n",
    "train_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Species_Beam</th>\n",
       "      <th>Species_Bream</th>\n",
       "      <th>Species_Parkki</th>\n",
       "      <th>Species_Perch</th>\n",
       "      <th>Species_Pike</th>\n",
       "      <th>Species_Roach</th>\n",
       "      <th>Species_Smelt</th>\n",
       "      <th>Species_Whitefish</th>\n",
       "      <th>Species_nan</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Species_Beam  Species_Bream  Species_Parkki  Species_Perch  Species_Pike  \\\n",
       "0           0.0            1.0             0.0            0.0           0.0   \n",
       "1           0.0            0.0             0.0            0.0           1.0   \n",
       "2           0.0            0.0             0.0            0.0           0.0   \n",
       "3           0.0            0.0             0.0            1.0           0.0   \n",
       "4           0.0            0.0             1.0            0.0           0.0   \n",
       "\n",
       "   Species_Roach  Species_Smelt  Species_Whitefish  Species_nan  \n",
       "0            0.0            0.0                0.0          0.0  \n",
       "1            0.0            0.0                0.0          0.0  \n",
       "2            0.0            1.0                0.0          0.0  \n",
       "3            0.0            0.0                0.0          0.0  \n",
       "4            0.0            0.0                0.0          0.0  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_column_names = ohe.get_feature_names_out(train_cat_data.columns)\n",
    "train_ohe = pd.DataFrame(train_ohe, columns=ohe_column_names)\n",
    "test_ohe = pd.DataFrame(test_ohe, columns=ohe_column_names)\n",
    "train_ohe.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 8: Fit The Model!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression Model Part\n",
    "\n",
    "    IMPORTANT: We ONLY fit the TRAINING data NOT testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import LinearRegression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create an instance of Linear Regression\n",
    "linear_reg_model = LinearRegression()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this step: the model is being fit aka \"trained\" on the present data. \n",
    "    \n",
    "    Initialization: preprocessor is an instance of the ColumnTransformer (or a similar preprocessing transformer). Before it can be used to transform data, it needs to be configured with the transformations you want to apply to your data.\n",
    "\n",
    "    Configuration: When you create a ColumnTransformer, you specify what preprocessing steps should be applied to different subsets of your features. For example, you might specify that numeric features should be scaled, while categorical features should be one-hot encoded. This configuration is based on the transformers you defined.\n",
    "\n",
    "    Fitting: The fit method of the ColumnTransformer goes through each specified transformation and \"fits\" or \"learns\" parameters from the training data. This step varies depending on the specific transformers used. For example:\n",
    "\n",
    "        If you're using StandardScaler to scale numeric features, it calculates the mean and standard deviation of each numeric feature from X_train.\n",
    "\n",
    "        If you're using OneHotEncoder to one-hot encode categorical features, it determines the unique categories in each categorical feature from X_train.\n",
    "\n",
    "    Parameter Learning: During the fitting process, the transformers are essentially learning something about the training data that they will later use to transform the data. For example, the scaler learns the mean and standard deviation of numeric features, while the one-hot encoder learns the unique categories.\n",
    "\n",
    "    Transformation: After fitting, the transformers are ready to transform new data. You can use the same preprocessor to transform both your training and test data. The learned parameters are applied consistently to ensure that the same transformations are performed on both datasets.\n",
    "\n",
    "In summary, preprocessor.fit(X_train) configures and learns the preprocessing steps you've specified in the ColumnTransformer based on the training data. This ensures that the same transformations are applied consistently to both the training and test data, preventing data leakage and allowing your model to make predictions on new, unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer()),\n",
       "                                                 (&#x27;scaler&#x27;, StandardScaler())]),\n",
       "                                 [&#x27;Length1&#x27;, &#x27;Length2&#x27;, &#x27;Length3&#x27;, &#x27;Height&#x27;,\n",
       "                                  &#x27;Width&#x27;]),\n",
       "                                (&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;ohe&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False))]),\n",
       "                                 [&#x27;Species&#x27;])])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">ColumnTransformer</label><div class=\"sk-toggleable__content\"><pre>ColumnTransformer(transformers=[(&#x27;num&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;, SimpleImputer()),\n",
       "                                                 (&#x27;scaler&#x27;, StandardScaler())]),\n",
       "                                 [&#x27;Length1&#x27;, &#x27;Length2&#x27;, &#x27;Length3&#x27;, &#x27;Height&#x27;,\n",
       "                                  &#x27;Width&#x27;]),\n",
       "                                (&#x27;cat&#x27;,\n",
       "                                 Pipeline(steps=[(&#x27;imputer&#x27;,\n",
       "                                                  SimpleImputer(strategy=&#x27;most_frequent&#x27;)),\n",
       "                                                 (&#x27;ohe&#x27;,\n",
       "                                                  OneHotEncoder(handle_unknown=&#x27;ignore&#x27;,\n",
       "                                                                sparse_output=False))]),\n",
       "                                 [&#x27;Species&#x27;])])</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">num</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Length1&#x27;, &#x27;Length2&#x27;, &#x27;Length3&#x27;, &#x27;Height&#x27;, &#x27;Width&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">cat</label><div class=\"sk-toggleable__content\"><pre>[&#x27;Species&#x27;]</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SimpleImputer</label><div class=\"sk-toggleable__content\"><pre>SimpleImputer(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneHotEncoder</label><div class=\"sk-toggleable__content\"><pre>OneHotEncoder(handle_unknown=&#x27;ignore&#x27;, sparse_output=False)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "ColumnTransformer(transformers=[('num',\n",
       "                                 Pipeline(steps=[('imputer', SimpleImputer()),\n",
       "                                                 ('scaler', StandardScaler())]),\n",
       "                                 ['Length1', 'Length2', 'Length3', 'Height',\n",
       "                                  'Width']),\n",
       "                                ('cat',\n",
       "                                 Pipeline(steps=[('imputer',\n",
       "                                                  SimpleImputer(strategy='most_frequent')),\n",
       "                                                 ('ohe',\n",
       "                                                  OneHotEncoder(handle_unknown='ignore',\n",
       "                                                                sparse_output=False))]),\n",
       "                                 ['Species'])])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit on train ONLY\n",
    "preprocessor.fit(X_train)\n",
    "# X_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform train and test data\n",
    "X_train_processed = preprocessor.transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 9: Inspect the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Species_Beam',\n",
       " 'Species_Bream',\n",
       " 'Species_Parkki',\n",
       " 'Species_Perch',\n",
       " 'Species_Pike',\n",
       " 'Species_Roach',\n",
       " 'Species_Smelt',\n",
       " 'Species_Whitefish',\n",
       " 'Species_nan']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# categorical_columns\n",
    "# # Fit the encoder on your categorical columns in your training data 'X_train'\n",
    "# ohe.fit(X_train[categorical_columns])\n",
    "\n",
    "# # Get the feature (column) names after one-hot encoding\n",
    "# ohe_cat_col_names = ohe.get_feature_names_out(input_features=categorical_columns)\n",
    "ohe_cat_col_names = ohe_column_names.tolist() # converting the Numpy Array to a list\n",
    "ohe_cat_col_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Check data types in numeric_columns\n",
    "print(type(numeric_columns))\n",
    "print(type(ohe_cat_col_names))\n",
    "print(type(X_train_processed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspectResults(train_data, test_data):\n",
    "    print(np.isnan(train_data).sum().sum(), 'missing values in training data')\n",
    "    print(np.isnan(test_data).sum().sum(), 'missing values in testing data')\n",
    "    print('\\n')\n",
    "    print('All data in X_train_processed are', train_data.dtype)\n",
    "    print('All data in X_test_processed are', test_data.dtype)\n",
    "    print('\\n')\n",
    "    print('shape of data is', train_data.shape)\n",
    "    print('\\n')\n",
    "    # Combine numeric and one-hot encoded column names\n",
    "    # Create a DataFrame with the correct number of columns\n",
    "    # train_data = pd.DataFrame(train_data, columns=numeric_columns + ohe_cat_col_names)\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 missing values in training data\n",
      "0 missing values in testing data\n",
      "\n",
      "\n",
      "All data in X_train_processed are float64\n",
      "All data in X_test_processed are float64\n",
      "\n",
      "\n",
      "shape of data is (119, 13)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.5790393 ,  0.66815832,  0.87988819, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [ 1.58547083,  1.65086656,  1.58672453, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-1.63511008, -1.73947686, -1.89014612, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.3173671 ,  0.37334585,  0.55512555, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.57835697, -0.56022698, -0.70571766, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.10533415, -0.08852702, -0.25678106, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspectResults(X_train_processed, X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor #baseline model for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Pipeline.__init__() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m dummy_reg \u001b[39m=\u001b[39m DummyRegressor(strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39m# create model pipeline\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m dummy_pipe \u001b[39m=\u001b[39m Pipeline(preprocessor, dummy_reg) \u001b[39m#since we preprocessed or prepared the pipeline with cleaned data\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# this is where the model starts learning \u001b[39;00m\n\u001b[1;32m      8\u001b[0m dummy_pipe\u001b[39m.\u001b[39mfit(X_train, y_train)\n",
      "\u001b[0;31mTypeError\u001b[0m: Pipeline.__init__() takes 2 positional arguments but 3 were given"
     ]
    }
   ],
   "source": [
    "# instantiate a baseline model\n",
    "dummy_reg = DummyRegressor(strategy='mean')\n",
    "\n",
    "# create model pipeline\n",
    "dummy_pipe = Pipeline(preprocessor, dummy_reg) #since we preprocessed or prepared the pipeline with cleaned data\n",
    "\n",
    "# this is where the model starts learning \n",
    "dummy_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluating your model. \n",
    "    Function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_regression(true,predicted_values):\n",
    "    ''' Takes true and predicted values (arrays) and prints MAE, MSE, RMSE, and R2'''\n",
    "    # don't need to use numpy to do these calculations\n",
    "    # taking 2 parameters because that's what is need the true values and predicted values for calculations\n",
    "    mae = mean_absolute_error(true, predicted_values)\n",
    "    mse = mean_squared_error(true, predicted_values)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(true, predicted_values)\n",
    "    print(f'MAE {mae}, \\n MSE {mse}, \\n RMSE {rmse}, \\n R^2 {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
